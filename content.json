{"pages":[{"title":"About me","text":"HiI am Yilin. :) bear logo Icons made by Good Ware from www.flaticon.com","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Config My Blog 01","text":"前言最近看很多大神同學都推薦 hexo 這個 Blog 框架，一時興起想說那就來寫一下 blog 吧！記得之前只有在國高中的時候用過無名寫過文章，還蠻好奇以前的自己究竟寫了什麼東西 XD。因為覺得最近的自己腦袋蠻空泛的，希望藉著寫一些部落格文章來 refresh 一下，同時也記錄各種生活中的大小事。 這次使用了 Hexo 這個 blog 框架。 Prerequisite Node.js Git Installation$ npm install -g hexo-cli Create a blog建立 hexo directory、並在此 directory 安裝好 npm 套件。 123$ hexo init blog$ cd blog$ npm install Components 待補充完整 _config.yml 網頁的整個配置設定。 source/ _posts 是放部落格文章的地方。 themes/ blog 的主題樣式。 ConfigTheme設置部落格主題為 next。 12$ cd blog$ git clone https://github.com/theme-next/hexo-theme-next themes/next 其中有四種 schema 可以選。在 theme 中的 _config.yml 搜尋 scheme 選擇想要的即可，在這邊選擇了 Pisces。 其中 code highlight theme 也可以選。我選擇將 highlight_theme: normal 改成 highlight_theme: night eighties。 在 theme 的 _config.yml 中把 social 的 github 打開、並且 enable social icon only。把 lazyload 設為 true、把 toc 設為 false（文章目錄拔掉）、把 updated_at 設為 false（編輯文章的時間拔掉）。 最後在 blog 的 _config.yml 將 theme: landscape 改成 theme: next 即可。 Categories寫文章前先建立 blog 的 “分類”。之後寫文章時可以把文章歸類。 1$ hexo new page Categories 就可以去 /blog/source/Categories/index.md 編輯。增加頁面的 type 屬性為 “categories”。可以把頁面的 comments 關閉。 123456---title: Categoriesdate: 2019-03-19 12:12:12type: &quot;categories&quot;comments: false--- 再來，就去 theme 的 _config.yml 編輯，把前方 categories 的註解#拿掉。我這邊也順便把 archives 加上註解 :P。 123456789menu:home: / || home#about: /about/ || user#tags: /tags/ || tagscategories: /categories/ || th#archives: /archives/ || archive#schedule: /schedule/ || calendar#sitemap: /sitemap.xml || sitemap#commonweal: /404/ || heartbeat DisqusDisqus 這個 plugin 為文章加入了留言的功能。 先去 Disqus 申請帳號。 申請 site。 到 site 的 settings 點進 general 查看自己 website 的 shortname。 修改 theme 的 _config.yml。1234disqus:enable: trueshortname: {{shortname}}count: true Writing Post首先建立新的文章。 12$ cd blog$ hexo new blog01 就可以進去 /blog/source/_posts/blog01.md 編輯文章。最前面的是一些基本資訊，關於文章標題、分類、標籤等等。 12345678910---title: {{標題}}date: {{創立時間}}categories: {{分類}}keywords: {{關鍵字}}tags:- {{tag1}}- {{tag2}}- {{tag3}}--- 接著就剩下文章內容的部分了，要自行發揮 :P。 Preview and Deploy設定完環境、寫完文章之後，啟動 local server。 12$ cd blog$ hexo server 即可在 http://localhost:4000 看到自己的部落格預覽的樣子。（其實隨時在修改 blog 時就可以隨時開著頁面預覽了 :P） 確認預覽的頁面為自己想要的樣子之後，接著就要 deploy 到 github page 上了。 首先修改 blog 的 _config.yml。 12345678language: zh-twurl: https://ylkuo96.github.io/deploy: type: git repo: https://github.com/ylkuo96/ylkuo96.github.io.git branch: master 部屬前要先安裝套件。 12$ cd blog$ npm install hexo-deployer-git --save 接著執行下列指令。 12$ cd blog$ hexo deploy note:: 若遇到 fatal: Not a git repository (or any of the parent directories): .git，將 blog 底下的 .deploy_git 資料夾刪掉然後重新 deploy 即可。 完成之後即可在 https://ylkuo96.github.io 上看到自己的 blog！ Summary 新文章就 $ hexo new &lt;postname&gt; 新分頁就 $ hexo new page &lt;pagename&gt; 產生靜態檔 $ hexo g (generate) 預覽 $ hexo s (server) 部屬 $ hexo d (deploy) 若有點怪怪的可以時不時 $ hexo clean 一下 :P References other people’s hexo blog 用 git 維持 hexo 資料夾 若 hexo g 報錯","link":"/2020/03/03/blog01/"},{"title":"Machine Learning Note 01","text":"前言這個學期下定決心要跟著進度把李宏毅機器學習影片看完並且寫完 15 個作業！於是想說可以邊寫個筆記來記錄一下 :P還有揪了以前交大的同學一起努力一起彼此督促，避免耍廢。 先貼一下這學期的影片進度與作業對照表。 以下是此篇筆記的影片連結： Introduction Regression Basic Concept 接下來就是筆記的內容了！ What is machine learning? 丟輸入給機器，機器找一個合適的 function、並輸出答案。 幾個機器學習的例子： Speech Recognition: 輸入聲音訊號、輸出字 Image Recognition: 輸入圖片、輸出這張圖有什麼物件 Playing GO: 輸入棋盤上黑子白子現在位址、輸出下一步該下什麼位址 Dialogue System: 輸入對機器說的話、輸出回應 Function Type Regression 讓機器找一個數值 ex. 預測隔天某時段的 PM2.5 Classification 讓機器做一個選擇 (從兩個或多個 option 中) binary 輸出只有兩種可能 (yes/no) ex. 輸入句子，輸出此句意義是正面/負面 multi-class 有 N 種選項，輸出會是其中一種 ex. 輸入圖片，輸出這是哪種類型的食物 Generation 讓機器學習創造 生成比較複雜、有結構的物件 ex. 輸出為翻譯、畫圖等等 Learning Type Supervised Learning 給機器 labeled training data、以及其 data 對應的答案 給一 loss function $L$，讓機器評估 function 的好壞 loss 越小越好，代表越接近理想的 function 機器會自動找出 loss 最低的 function Reinforcement Learning 不需要給機器正確解答，讓機器自己去摸索。根據學習過程中得到的 feedback 來調整學習策略。 ex. 預測圍棋下一步位置： 不需要告訴機器每一個情況最正確落子的位址 讓機器跟自己下或是機器跟別人下 機器自己找出適合的策略 若輸了，機器要自己想辦法去提高正確率 機器輸或贏會是 reward，此 reward 可以引導機器學習的方向 Unsupervised Learning 給機器一堆 unlabeled data 去學 :P How can machine find out our expected function?給一函式尋找範圍（Network Atchitecture），讓機器在此範圍搜索就好。像是只考慮 Linear Function (regression or classification) 或是只考慮 Neural Network (RNN or CNN)。並且使用 Gradient Descent 此種函示尋找方法。 Research Explainable AI 機器解釋為甚麼選這個答案 Adversarial Attack 如果惡意攻擊自動辨識系統會怎樣 故意增加些刻意的雜訊、針對機器的雜訊，導致機器可能會辨識錯誤 Network Compression 有個 model 可以辨識很高的正確率，但他很肥 要怎麼縮小 network，使得可以放到手機/edge device 上 Anomaly Detection 如果放不同種類的東西、怪怪的東西給系統會怎樣 ex. 放一個動漫人物的圖片給一個專門辨識動物的機器，機器能不能回答：我不知道 Domain Adversarial Learning 如果訓練資料跟測試資料很像（distribution 很像），那正確率會很高 =&gt; 假象 :P 那如果訓練資料跟測試資料分布不一樣的話要怎麼做才能保持一樣高的正確率？ Meta Learning Learn to learn =&gt; 學習如何學習的能力 ex. 讓程式寫另一個程式/發明另一個演算法 現今人類設計的機器學習的方式很沒效率，所以期望機器自己找到有效率的方法 Life-long Learning 終身學習，同樣叫做 Continuous Learning, Never Ending Learning, Incremental Learning 讓機器終身學習、不斷學習下去 學好一個東西就繼續學下一個 Regression這邊舉一些 Regression 的例子： 股票預測：輸入過去十年各種股票起伏資料等等、輸出明天某股票指數 無人車、自動車：輸入各種 sensor 的 info、輸出方向盤角度 推薦系統：輸入使用者A和商品B、輸出為使用者A購買商品B的可能性 預測寶可夢進化後的 CP 值：輸入某一隻寶可夢、輸出其進化後的 CP 值 以下用寶可夢作為例子：）$f(pokemon) = y$ $x_{cp}$：寶可夢進化前的 CP 值$x_s$：寶可夢屬於哪個物種 ex. 妙挖種子$x_{hp}$：寶可夢生命值$x_w$：寶可夢的重量$x_h$：寶可夢的高度$y$：進化後的 CP 值 Machine Learning Three Steps 機器學習很 basic 的三步驟！ 找一 model/function set 定義 function set 中某一 function，並估算其好壞 找出最好的 function (I) Find a Function Set 要找 function set (model) Q：那，這個 model 應該要長怎樣呢？A：應該要是 input 為寶可夢，output 為進化後的 CP 值的 function 這邊先假設一個很簡單的 model：$y = b + w \\cdot x_{cp}$ 其中 w 與 b 這兩個參數可以是任何 value 填入不同的數字，就得到不同的 function :P ex. $f_1 = 10 + 9.0 \\cdot x_{cp}$、$f_2 = 9.8 + 9.2 \\cdot x_{cp}$ 有 $\\infty$ 種可能的 function 這個很簡單的 model 為 Linear Model：$y=b+\\sum{w_ix_i}$ $x_i:$ an attribute of input x（feature） $w_i$ 是 weight $b$ 是 bias Collecting training data 再找 function 之前，要先蒐集 training data Squirtle =&gt; Wartortle input $x^1$ 為 Squirtle；output $\\hat{y}^1$ 為 979 Eevee =&gt; Sparky input $x^2$ 為 Eevee；output $\\hat{y}^2$ 為 1420 …….. 假設蒐集到 10 隻 pokemon 的 data 有了 training data，就要定義 function 的好壞。 (II) Goodness of Function 定義 function 好壞 定義 Loss Function $L$：$L(f)=L(w,b)$ input: a function output: how bad this function is 也就是說，loss function 在衡量參數的好壞。 可以自己定義 loss function 的，但是有比較常見的作法 — 估測誤差：$L(f)=\\sum\\limits_1^{10} {(\\hat{y}^n - (b + w * x_{cp}^n))^2}$ $\\hat{y}^n:$ the real y $(b+w \\cdot x_{cp}^n):$ estimated y based on input function (III) Best Function 在上一步驟已經定好 loss function，可以衡量每一個 function 的好壞了接下來，就是要從這個 set 裡面，挑選出最好的一個 function $f^* = \\arg\\min\\limits_f L(f)$$w^*, b^* = \\arg\\min\\limits_{w,b} L(w,b))$ 所以，要做的事情是就是找到一組 $w$ &amp; $b$ 使得 loss 為最小。 用線性代數可以找到 Closed-Form Solution 或是用 Gradient Descent Gradient Descent 只要 L 可微分，就能拿來找可能比較好的 solution 首先，先來看比較簡單的情況。若 loss function 只有一個參數，Gradient Descent 要如何求解？ a) Loss function with only one parameter $w$ $w^* = \\arg \\min\\limits_w L(w))$ 以下步驟： Randomly pick an initial value $w^0$ Compute $\\frac{dL}{dw}|_{w= w^0}$ 計算切線斜率 若為負，表示左邊 loss 比右邊 loss 高 =&gt; 增加 $w$ 值 若為正，表示右邊 loss 比較高 =&gt; 減少 $w$ 值 Update: $w^1 \\gets w^0 - \\eta *\\dfrac{dL}{dw_0}$ Repeat 2), 3) … Compute $\\frac{dL}{dw}|_{w=w^1}$ Update: $w^2 \\gets w^1 - \\eta *\\dfrac{dL}{dw_1}$ … Continue T times, then get the local min :) 以上更新的幅度有多大取決於兩件事情： 微分值 $\\frac{dL}{dw}$ 有多大 如果越大表示越陡峭，則移動距離越大；反之越小 常數項 learning rate $\\eta$ 是事先定好的數字；若定越大，參數更新的幅度就越大 經過非常多次 iteration 更新參數後，會到 local min 的地方： 也就是微分是 0 的地方 但幸運的是，在 linear regression 上沒有 local min :P 若 $L$ 有兩個參數呢 $w, b$ ？ 在進入情境 b 之前，先複習一下偏微分ㄅㄅㄅ :P由於 $L = \\sum{(\\hat{y}^n - (b + w * x_{cp}^n))^2}$，所以： $w$ 對 $L$ 的偏微分： $\\dfrac{\\partial L}{\\partial w} \\vert_{w=w^0,b=b^0} = \\sum{2(\\hat{y}^n - (b + w * x_{cp}^n))*(-x_{cp}^n)}$ $b$ 對 $L$ 的偏微分： $\\dfrac{\\partial L}{\\partial b}\\vert_{w=w^0,b=b^0} = \\sum{2(\\hat{y}^n - (b + w * x_{cp}^n))*(-1)}$ Q：那，Gradient $\\nabla L$ 是什麼呢？A：把 $w$ 對 $L$ 的偏微分和 $b$ 對 $L$ 的偏微分排成一個 vector b) Loss function with two parameters $w, b$ $w^*, b^* = \\arg \\min\\limits_{w,b} L(w,b))$ 以下步驟其實大同小異： Randomly pick an initial value $w^0, b^0$ Compute $\\dfrac{\\partial L}{\\partial w}|_{w=w^0,b=b^0}$ $\\dfrac{\\partial L}{\\partial b}|_{w=w^0,b=b^0}$ Update $w^1 \\gets w^0 - \\eta * \\dfrac{\\partial L}{\\partial w}|_{w=w^0,b=b^0}$ $b^1 \\gets b^0 - \\eta * \\dfrac{\\partial L}{\\partial b}|_{w=w^0,b=b^0}$ Repeat 2), 3) … Compute: $\\dfrac{\\partial L}{\\partial w}|_{w=w^1,b=b^1}$ $\\dfrac{\\partial L}{\\partial b}|_{w=w^1,b=b^1}$ Update: $w^2$ and $b^2$ … Continue T times, then get the local min :D 關於 local min 的問題：Q：Gradient Descent 可以找到 local min，但會不會找不到 global min？A：但是！In linear regression, loss function $L$ 是 convex 的，它並沒有 local min optimal，他就是 global min！ Improvement 得到 model 之後，使用在 testing data，並試著改善。 Model Complexity假設經過上述 3 步驟後，得到最終 model 為 $y=-188.4+2.7 \\cdot x_{cp}$。 我們又抓了另外 10 隻 pokemon 當成 testing data，然後計算每個 data point 離 function 最近的距離 $e^i$，也就是 error。 假設： average error on training data $= 31.9$ average error on testing data $= \\sum\\limits_n^{10}e^n = 35.0$ 其實可以看的出來 testing data 與 training data 的分布是差不多的，因為 performance 感覺起來是不相上下的。 從下圖（testing data）結果看來，model 可能不是直線這麼簡單，可能需要改善。 Q：那要如何改善呢？A：需要複雜一點的 model，像是引入二次式之類的。 於是重新設計 model 為 $y=b+w_1 \\cdot x_{cp}+w_2 \\cdot (x_{cp})^2$。然後再重複剛剛的步驟，用 Gradient Descent 找出最好的參數。 假設，找到的最好的 function 是 $y = -10.3 + x_{cp} + 2.7 \\cdot 10^{-3} \\cdot (x_{cp})^2$，並計算 testing data 的 error。 average error on training data $= 15.4$ average error on testing data $= 18.4$ 觀察下圖（testing data）得知，testing data 與二次式更 fit 一點。 但是！！也不能太太太複雜，否則會適得其反。 像是若重新設計 model 為四次式 $y=b+w_1 \\cdot x_{cp}+w_2 \\cdot (x_{cp})^2 + w_3 \\cdot (x_{cp})^3 + w_4 \\cdot (x_{cp})^4$，找到的最好 model 之後，並計算 error。 average error on training data $= 14.9$ average error on testing data $= 28.8$ 下圖為 model complexity 與 error 之間的比較。 可以看到 model 複雜度越高，對 training data 的 error 會越低，但對 testing data 不減反增，這就是 Overfitting。 所以，選擇適當的 model 就好，不需要太複雜。 Hidden Factors蒐集了更多筆資料之後，可以發現，物種這個 factor 深深影響著 output，每個物種的 model 貌似長的都不太一樣。 只考慮 $x_{cp}$ 是不夠的，於是重新設計 model。 if $x_s =$ Pidgey: $y = b_1 + w_1 \\cdot x_{cp}$ if $x_s =$ Weedle: $y = b_2 + w_2 \\cdot x_{cp}$ if $x_s =$ Caterpie: $y = b_3 + w_3 \\cdot x_{cp}$ if $x_s =$ Eevee: $y = b_4 + w_4 \\cdot x_{cp}$ Q：阿要怎麼把 if 放到 linear model 裡面？A：改寫成以下 linear function $y = b_1 \\cdot \\delta(x_s = Pidgey) + w_1 \\cdot \\delta(x_s = Pidgey) \\cdot x_{cp}$$\\mbox{ }\\mbox{ }\\mbox{ } +b_2 \\cdot \\delta(x_s = Weedle) + w_2 \\cdot \\delta(x_s = Weedle) \\cdot x_{cp}$$\\mbox{ }\\mbox{ }\\mbox{ } +b_3 \\cdot \\delta(x_s = Caterpie) + w_3 \\cdot \\delta(x_s = Caterpie) \\cdot x_{cp}$$\\mbox{ }\\mbox{ }\\mbox{ } +b_4 \\cdot \\delta(x_s = Eevee) + w_4 \\cdot \\delta(x_s = Eevee) \\cdot x_{cp}$ 其中 delta function $\\delta$ 的值為 1, if $x_s$ = Pidgey 0, otherwise 假設目前物種為 Pidgey，則看到的 function 會為 $y = b_1 + w_1 \\cdot x_{cp}$；也就是各物種的 function 都不一樣。 再用 Gradient Descent 找到最好的參數、並且計算 error： average error on training data $= 3.8$ average error on testing data $= 14.3$ 結果為下圖（testing data），比剛剛的結果都更好。 但是好像還有其他地方能夠改善，可能會是 $x_h, x_w, x_{hp}$ 等等其他的因素。如果沒有 domain knowledge，不知道那個 attribute 比較跟結果有關，則可以通通丟進去設計 model。 if $x_s =$ Pidgey: $y’ = b_1 + w_1 \\cdot x_{cp} + w_5 \\cdot (x_{cp})^2$ if $x_s =$ Weedle: $y’ = b_2 + w_2 \\cdot x_{cp} + w_6 \\cdot (x_{cp})^2$ if $x_s =$ Caterpie: $y’ = b_3 + w_3 \\cdot x_{cp} + w_7 \\cdot (x_{cp})^2$ if $x_s =$ Eevee: $y’ = b_4 + w_4 \\cdot x_{cp} + w_8 \\cdot (x_{cp})^2$ 得到最終 model 為： $y = y’ + w_9 \\cdot x_{hp} + w_{10} \\cdot (x_{hp})^2 + w_{11} \\cdot x_h + w_{12} \\cdot (x_h)^2 + w_{13} \\cdot x_w + w_{14} \\cdot (x_w)^2$ 計算 error： average error on training data $= 1.9$ average error on testing data $= 102.3$ 在 testing data 的結果太糟糕啦～～～～～可以用 regularization 解決！ Regularization 重新定義 loss function $L$，把一些 knowledge 放進去，找到比較好的 function。 一開始的 loss function 只考慮了 prediction 的 error 這件事：$y = b + \\sum w_i x_i$$L = \\sum\\limits_n(\\hat{y}^n - (b + \\sum w_i x_i))^2$ Regularization 就是加上 額外的 term 來改善 loss function：$L = \\sum\\limits_n \\left ( \\hat{y}^n - \\left ( b + \\sum w_i x_i \\right ) \\right )^2 +$ $\\lambda \\sum (w_i)^2$ 現在分別看兩項的意思： prediction error：很直觀，錯誤越少越好 $\\lambda \\sum (w_i)^2$：參數的值越小越接近 0 越好 Q：WHY 參數越小越好？A：參數越接近 0，function 也就越平滑，也就是說 output 對 input 的變化越不敏感。 $y = b + \\sum w_i x_i$當 input 增加了 $\\Delta x_i$，output 會增加 $w_i \\Delta x_i$。所以當 input 改變，$w_i$ 越小，output 越不容易有變化。 Q：為何喜歡比較 smooth 的 function？A：假設 noise 影響了 input，function 會受到較少的影響，給我們比較好的結果。 下圖為加入 regularization 的實驗結果： 當 $\\lambda$ 越大，考慮 smooth 那項 term 的影響力越大，所以找到的 function 就越平滑。 當 $\\lambda$ 越大，會呈現兩種現象： training data error 越大 testing data error 可能越小 但太大又會反升，例如平滑到像是一條直線 :O Q：為何 function 越平滑，training data error 會越大？A：因為越傾向於考慮 w 的數值，減少考慮 error。 Q：function 要有多 smooth？A：調 $\\lambda$ 得到最好的 model note:: 在做 regularization 的時候不用考慮 bias $b$。因為調整 bias 值只是上下移動 function，對平滑值沒有關係。 Where does the error come from? error 有兩個來源：bias、variance所以要找出 error 來源，才能對症下藥！ 只有 God 知道理想的 function $\\hat{f}$ 長怎樣，而我們只能從 traning data 找到我們理想中的 $f^*$。 所以 $\\hat{f}$ 與 $f^*$ 之間的差距就是 error，來自於 bias 或 variance。 Bias and Variance of Estimator Bias of Estimator 假設現在有一 variable $x$： the mean of $x$ is $\\mu$ the variance of $x$ is $\\delta^2$ 若要估測 mean $\\mu$ 的話： 首先先 sample N 個點: ${x^1, x^2, …, x^N}$ 再算 N 個點的平均值 $m = \\frac{1}{N} \\sum\\limits_n x^n \\neq \\mu$ （不見得一樣） 於是做重複多次1. 2. 步驟，得到多次 $m_i$ 再算期望值 $E[m] = E[\\frac{1}{N} \\sum\\limits_n x^n] = \\frac{1}{N} \\sum\\limits_nE[x^n] = \\mu$ 所以可以用 m 來 estimate $\\mu$，m 是 unbiased 的，因為其期望值會正好與 $\\mu$ 相等！ 若 N 較大，m 會較集中；反之較分散。 Variance of Estimator 若要估測 variance $\\delta^2$ 的話： $s^2 = \\frac{1}{N} \\sum\\limits_n (x^n-m)^2$ 同上面 sample 多次、得到多個 m、算出多個 $s_i$ 期望值 $E[s^2] = \\frac{N-1}{N} \\delta^2 \\neq \\delta^2$ $s^2$ 為 biased estimator，因為其期望值並不會等於$\\delta^2$。 若 N 較大，$s^2$ 與 $\\delta^2$ 其之間差距會更小。 Estimate the real function $\\hat{f}$取決於兩件事： 是否瞄準正中心 =&gt; bias 做多次實驗得到多個 $f^*$，算出期望值 $E[f^*] = \\bar{f}$ 若 $\\bar{f}$ 不是散佈於 $\\hat{f}$ 附近的話代表有偏差 bias。 偏移 =&gt; variance 每次 $f^*$ 與 $\\bar{f}$ 的距離 有四種情況： 左上：bias 小、vaiance 小。 此為最理想的狀況。 左下：bias 大、variance 小。 每次找的 $f^*$ 都很像，但都集中在錯誤位置。 右上：bias 小、variance 大。 瞄準對，但每次 $f^*$ 都差很多。 右下：bias 多、variance 大。 Model vs Variance 以 model 複雜度來說，越簡單 variance 越小。 ex. $y = b + w * x_{cp}$ vs $y = b + w_1 * x_{cp} + w_2 * (x_{cp})^2 …$ Q：WHY?A：因為越簡單的 model 受到不同 data 的影響較小。 Model vs Bias 以 model 複雜度來說，越簡單 bias 越大。 $E[f^*] = \\bar{f}$ 若 bias 大，表示 $\\bar{f}$ 與 $\\hat{f}$ 有距離。若 bias 小，表示 $\\bar{f}$ 與 $\\hat{f}$ 是靠近的。 black curve: the true function $\\hat{f}$ red curves: 5000 $f^*$ blue curve: average of 5000 $f^* = \\bar{f}$ 若為一次方的 model： $\\bar{f}$ 與 $\\hat{f}$ 較遠。 若為五次方的 model： $\\bar{f}$ 與 $\\hat{f}$ 是接近的。 Model vs Variance vs Bias複雜度較小的 model： bias 大、但 variance 小 複雜度較大的 model： bias 小、但 variance 大 因為 model 為一個範圍的 function set。當定義好一 model，就已經設定好說最好的 function 就只能從那個範圍內挑出來。 若 model 較簡單，space 較小，可能根本就沒有包含 target $\\hat{f}$。若 model 較複雜，space 較大，比較有機會包含到 target $\\hat{f}$。 總結為下圖： Overfitting若 error 來自於 variance 大： Overfitting If can fir the training data, but large error on testing data, then probably have large variance 解法： More data very effective, but not always practical :( Generate our own training data ex. image recognition：把圖片旋轉，就多一倍的 data 了 ex. speech recognition：變聲器翻轉男女聲 ex. language understanding task：英文翻成中文 :P Regularization 在原來的 loss function 裡面多加一項，希望參數越小越好、曲線越 smooth 越好 但 bias 可能會變大，所以 weight 要調好，要取得 bias 與 variance 間的平衡 Underfitting若 error 來自於 bias 大： Underfitting If model cannot even fit the training examples, then have large bias 解法： Redesign Model Add more features as input A more complex model Model Selection There is usually a trade-off between bias and varianceSelect a model that balances two kinds of error to minimize total error WHAT WE SHOULD NOT DO:我們不該拿手上有的 testing set 去 validate 我們的 model。因為自己手上擁有的 testing set 會有自己的一個 bias。 所以，要 Cross Validation！ Cross Validation把手中有的 training set 分成兩組： training set 得出 model1, model2, model3 validation set validate 出哪一個 model performance 較好 若假設得出最好的 model 是 model3，還可以 combine training set 與 validation set，並用在 model3 上再 train 一次。 接下來，就用 (public) tesint data 去測試。但不建議測完 tesing data 後回去 tune model，因為這樣又會把 testing data 的 bias 考慮進去。也就是說，此舉動是 make public testing set better than private testing set；在 public testing set 中的 performance 無法反映在 private testing set 上。 若擔心 validation set 怪怪的，有 bias，則可以做 N-fold Cross Validation。 N-fold Cross Validation把 training set 分成 N 份： training set (N-1) validation set (1) 一直輪流讓其中一份當 validation set。選出一 model 後，用完整的 training set 再去 train 一次此 model。 Example：假設現在有 model1, model2, model3，且現在使用 3-fold Cross Validation。data 被分為 [data0, data1, data2] 三份。 會有三種情況： train = [data0, data1]; val = [data2] train = [data0, data2]; val = [data1] train = [data1, data2]; val = [data0] 分別使用每一種 model，套用在這三種情形，得出每一個 model 的 average error。 假設 model1 表現最好，再用此完整的 training set 去 train model1。 最後，再去測 testing set。","link":"/2020/03/18/ml01/"},{"title":"Config My Blog 02","text":"前言看到外星人的部落格主題 hexo-icarus 覺得太好看了，樣式全白、簡單、又有質感！於是花了幾天把主題從 hexo-next 改成 hexo-icarus，然後也有對其原先設定的 layout 做了一些修改。 版面配置三欄因為原本主題設定是有三個欄位，中間的欄位負責 display 文章，但看起來有點沒有重點，而且不夠 basic，所以把三欄改為兩欄。這邊其實蠻簡單的，就是去 icarus/_config.yml 裡面把 widget 的 position 通通都設為 left，這樣原本在右邊的欄位就會通通跑到左邊來了。其中我只留下幾個 widget，分別是 profile、toc、category、recent_posts、archive，其他都註解掉了。 版面比例由於現在版面剩下兩欄，但其實左右兩側整個留空的比例還蠻高的，所以希望把這兩欄稍微往左右兩側擴展一些些。這邊對 column == 2 時做了修改。 icarus/layout/layout.ejs1234567891011&lt;% function main_column_class() { switch (column_count()) { case 1: return 'is-12'; case 2: return 'is-8-tablet is-9-desktop is-9-widescreen'; case 3: return 'is-8-tablet is-8-desktop is-6-widescreen' } return '';} %&gt; icarus/layout/common/widget.ejs123456789&lt;% function side_column_class() { switch (column_count()) { case 2: return 'is-4-tablet is-3-desktop is-3-widescreen'; case 3: return 'is-4-tablet is-4-desktop is-3-widescreen'; } return '';} %&gt; 把兩邊留空改小一點。 icarus/source/css/style.styl1gap = 15px 側邊欄近期文章原先 sidebar 近期文章的顯示是標題+文章日期+文章類別+照片，但覺得這樣蠻多餘的，於是把照片跟類別拿掉，只留下最重要的資訊。這邊註解了 image 的 block 以及 categories 的 block。（憑單詞註解:P） icarus/layout/widget/recent_posts.ejs12345678910111213141516&lt;!-- &lt;p class=&quot;image is-64x64&quot;&gt; &lt;img class=&quot;thumbnail&quot; src=&quot;&lt;%= post.thumbnail %&gt;&quot; alt=&quot;&lt;%= post.title %&gt;&quot;&gt; &lt;/p&gt;--&gt; &lt;!-- &lt;p class=&quot;is-size-7 is-uppercase&quot;&gt; &lt;%- list_categories(post.categories(), { show_count: false, class: 'has-link-grey ', depth:2, style: 'none', separator: ' / '}) %ʕ￫ᴥ￩ʔ&gt; &lt;/p&gt;--&gt; icarus/_config.yml 基本設定 只打算留下 navigation bar 的 home、categories、about。（menu） 把原本右上角連到 hexo-icarus github 的連結註解掉了。（links） 網站右下角原先有三個連結也註解掉了。（footer） 加入之前就有的留言/評論功能。（comment） 註解 donation 的所有部分。(donate) icarus/_config.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253menu: #Home: / #Archives: /archives Categories: /categories #Tags: /tags About: /about#links: #Download on GitHub: #icon: fab fa-github #url: 'https://github.com/ppoffice/hexo-theme-icarus'#footer: # Links to be shown on the right of the footer section #links: #Creative Commons: #icon: fab fa-creative-commons #url: 'https://creativecommons.org/' #Attribution 4.0 International: #icon: fab fa-creative-commons-by #url: 'https://creativecommons.org/licenses/by/4.0/' #Download on GitHub: #icon: fab fa-github #url: 'https://github.com/ylkuo96' #donate: #- # Donation entry name #type: alipay # Qrcode image URL #qrcode: '' #- # Donation entry name #type: wechat # Qrcode image URL #qrcode: '' #- # Donation entry name #type: paypal # Paypal business ID or email address #business: '' # Currency code #currency_code: USD #- # Donation entry name #type: patreon # URL to the Patreon page #url: ''comment: # Name of the comment plugin type: disqus shortname: yilin-comments 關於我這邊多創建了一個分頁，大概就是要介紹我自己的意思 XD！不過我還沒寫完，只是先擺著放而已 :P。$ hexo new page about 然後把評論功能關掉，只想開放文章的留言功能而已。 blog/source/about/index.md12345678---title: About medate: 2020-03-04 17:43:57type: \"about\"comment: false---&lt;&lt;待補充 :P&gt;&gt; favicon關於網站的 icon （分頁上的 icon），把它換成覺得很有質感的線條。用此網站製作而成的。這個線條叫做 wavy dash。 熊 logoicarus/layout/common/navbar.ejs12345678&lt;a class=&quot;navbar-item navbar-logo&quot; href=&quot;&lt;%- url_for('/') %&gt;&quot;&gt;&lt;% if (logo &amp;&amp; logo.text) { %&gt; &lt;img src=&quot;/images/bear.svg&quot; style=&quot;display:block;width:25px;height:25px&quot; /&gt; &lt;%= logo.text %&gt;&lt;% } else { %&gt; &lt;img src=&quot;&lt;%- url_for(logo) %&gt;&quot; alt=&quot;&lt;%= title %&gt;&quot; height=&quot;28&quot;&gt;&lt;% } %&gt;&lt;/a&gt; 多加了第三行的 &lt;img src ... &gt;，其中 bear.svg 被放置在 icarus/source/images/。完成之後會顯示如下圖。 超級無敵可愛的 logo 啊！！！！！連結點此。 最後順便改了一下 blog/source/about/index.md，標示一下此 logo 的 author。 References 外星人部落格 logo","link":"/2020/03/05/blog02/"},{"title":"Hello Again","text":"前言學期中太忙了就忘記更新 blog 了，連當初說要寫的機器學習筆記在看了幾部影片之後就直接暫停計畫 XD希望八月能繼續開始，期勉自己！ 碩一下學期課程總結這學期修了機器學技法 and 虛擬機器，都是很有趣的課程，學到的東西也超乎想像的多。開學前還想要把 VM 的筆記也更新上來 blog，如果之後忘記還可以 review，而且未來自己看的時候肯定很有成就感：P Diary學期忙完大概七月初到現在七月底，中間跟很多朋友約吃飯，包括最愛的吳留手；還常去長庚醫院看 father，好懷念全家人一起出遊的日子。只能說小時候太不懂事，爸爸媽媽拖我們出去玩還常常耍賴、結屎臉給他們看，現在很懷念。 中間還經歷光頭去當兵。當完兩個禮拜研替的兵還立刻馬上拉他去花蓮兩天一夜，買了很多伴手禮：P 我跟光頭的影子，順便曬一下我新買的 muji 小白鞋。 這個光、這個雲超美！！！ 七星潭～ at 瑞穗牧場。 大概就是這樣，滿滿的流水帳，打完收工。","link":"/2020/07/26/mood01/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"hexo-next","slug":"hexo-next","link":"/tags/hexo-next/"},{"name":"ml","slug":"ml","link":"/tags/ml/"},{"name":"note","slug":"note","link":"/tags/note/"},{"name":"hexo-icarus","slug":"hexo-icarus","link":"/tags/hexo-icarus/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"ml","slug":"ml","link":"/categories/ml/"},{"name":"mood","slug":"mood","link":"/categories/mood/"}]}