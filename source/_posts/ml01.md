---
title: Machine Learning Note 01
date: 2020-03-18 00:48:05
categories: ml
keywords: ml-note
tags: 
- ml
- note
---

## 前言
這個學期下定決心要跟著進度把李宏毅機器學習影片看完並且寫完 15 個作業！
於是想說可以邊寫個筆記來記錄一下 :P
還有揪了以前交大的同學一起努力一起彼此督促，避免耍廢。

<!--more-->

先貼一下這學期的[影片進度與作業對照表](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html)。
{% img /uploads/images/ml01/outline.png 800 'outline' %}

以下是此篇筆記的影片連結：
- [Introduction](https://www.youtube.com/watch?v=c9TwBeWAj_U&feature=youtu.be)
- [Regression](https://www.youtube.com/watch?v=fegAeph9UaA&feature=youtu.be)

皆下來就是筆記的內容了！

## What is machine learning?
> 丟輸入給機器，機器找一個合適的 function、並輸出答案。

幾個機器學習的例子：
1. Speech Recognition: 輸入聲音訊號、輸出字
2. Image Recognition: 輸入圖片、輸出這張圖有什麼物件
3. Playing GO: 輸入棋盤上黑子白子現在位址、輸出下一步該下什麼位址
4. Dialogue System: 輸入對機器說的話、輸出回應

## Function Type
1. Regression
    - 讓機器找一個數值
    - ex. 預測隔天某時段的 PM2.5
2. Classification
    - 讓機器做一個選擇 (從兩個或多個 option 中)
    - binary
        - 輸出只有兩種可能 (yes/no)
        - ex. 輸入句子，輸出此句意義是正面/負面
    - multi-class
        - 有 N 種選項，輸出會是其中一種
        - ex. 輸入圖片，輸出這是哪種類型的食物
3. Generation
    - 讓機器學習創造
    - 生成比較複雜、有結構的物件
    - ex. 輸出為翻譯、畫圖等等

## Learning Type
1. Supervised Learning
    - 給機器 labeled training data、以及其 data 對應的答案
    - 給一 loss function $L$，讓機器評估 function 的好壞
        - loss 越小越好，代表越接近理想的 function  
    - 機器會自動找出 loss 最低的 function

2. Reinforcement Learning
    - 不需要給機器正確解答，讓機器自己去摸索。根據學習過程中得到的 feedback 來調整學習策略。
    - ex. 預測圍棋下一步位置：
        - 不需要告訴機器每一個情況最正確落子的位址
        - 讓機器跟自己下或是機器跟別人下
        - 機器自己找出適合的策略
        - 若輸了，機器要自己想辦法去提高正確率
        - 機器輸或贏會是 reward，此 reward 可以引導機器學習的方向

3. Unsupervised Learning
    - 給機器一堆 unlabeled data 去學 :P

## How can machine find out our expected function?
給一函式尋找範圍（Network Atchitecture），讓機器在此範圍搜索就好。
像是只考慮 Linear Function (regression or classification) 或是只考慮 Neural Network (RNN or CNN)。
並且使用 Gradient Descent 此種函示尋找方法。

## 前沿研究
1. Explainable AI
    - 機器**解釋**為甚麼選這個答案
2. Adversarial Attack
    - 如果惡意攻擊自動辨識系統會怎樣
    - 故意增加些刻意的雜訊、針對機器的雜訊，導致機器可能會辨識錯誤
3. Network Compression
    - 有個 model 可以辨識很高的正確率，但他很肥
    - 要怎麼縮小 network，使得可以放到手機/edge device 上
4. Anomaly Detection
    - 如果放不同種類的東西、怪怪的東西給系統會怎樣
    - ex. 放一個動漫人物的圖片給一個專門辨識動物的機器，機器能不能回答：我不知道
5. Domain Adversarial Learning
    - 如果訓練資料跟測試資料很像（distribution 很像），那正確率會很高 => 假象 :P
    - 那如果訓練資料跟測試資料分布不一樣的話要怎麼做才能保持一樣高的正確率？
6. Meta Learning
    - Learn to learn => 學習如何學習的能力
    - ex. 讓程式寫另一個程式/發明另一個演算法
    - 現今人類設計的機器學習的方式很沒效率，所以期望機器自己找到有效率的方法
7. Life-long Learning
    - 終身學期，同樣叫做 Continuous Learning, Never Ending Learning, Incremental Learning
    - 讓機器終身學習、不斷學習下去
    - 學好一個東西就繼續學下一個


> 待補完 Regression ... :) 


## Where does the function come from?
> error 有兩個來源：bias、variance
> 所以要找出 error 來源，才能對症下藥！

只有 God 知道理想的 function $\hat{f}$ 長怎樣，而我們只能從 traning data 找到我們理想中的 $f^*$。

所以 $\hat{f}$ 與 $f^*$ 之間的差距就是 error，來自於 bias 或 variance。

## Bias and Variance of Estimator
> Bias of Estimator

假設現在有一 variable $x$：
- the mean of $x$ is $\mu$
- the variance of $x$ is $\delta^2$

若要估測 mean $\mu$ 的話：
1. 首先先 sample N 個點: ${x^1, x^2, ..., x^N}$
2. 再算 N 個點的平均值 $m = \frac{1}{N} \sum\limits_n x^n \neq \mu$ （不見得一樣）
3.  於是做重複多次1. 2. 步驟，得到多次 $m_i$
4.  再算期望值 $E[m] = E[\frac{1}{N} \sum\limits_n x^n] = \frac{1}{N} \sum\limits_nE[x^n] = \mu$

所以可以用 m 來 estimate $\mu$，m 是 unbiased 的，因為其期望值會正好與 $\mu$ 相等！

若 N 較大，m 會較集中；反之較分散。

> Variance of Estimator

若要估測 variance $\delta^2$ 的話：
1. $s^2 = \frac{1}{N} \sum\milits_n (x^n-m)^2$
2. 同上面 sample 多次、得到多個 m、算出多個 $s_i$
3. 期望值 $E[s^2] = \frac{N-1}{N} \delta^2 \neq \delta^2$

$s^2$ 為 biased estimator，因為其期望值並不會等於$\delta^2$。

若 N 較大，$s^2$ 與 $\delta^2$ 其之間差距會更小。

## Estimate $\hat{f}$
取決於兩件事：
1. 是否瞄準正中心 => bias
    - 做多次實驗得到多個 $f^*$，算出期望值 $E[f^*] = \bar{f}$
    - 若 $\bar{f}$ 不是散佈於 $\hat{f}$ 附近的話代表有偏差 bias。
2. 偏移 => variance
    - 每次 $f^*$ 與 $\bar{f}$ 的距離

{% img /uploads/images/ml01/bv.png 800 'bias and variance' %}
有四種情況：
- 左上：bias 小、vaiance 小。
    - 此為最理想的狀況。
- 左下：bias 大、variance 小。
    - 每次找的 $f^*$ 都很像，但都集中在錯誤位置。
- 右上：bias 小、variance 大。
    - 瞄準對，但每次 $f^*$ 都差很多。
- 右下：bias 多、variance 大。

## Model vs Variance
> 以 model 複雜度來說，越簡單 variance 越小。

ex. $y = b + w * x_{cp}$ vs $y = b + w_1 * x_{cp} + w_2 * (x_{cp})^2 ...$

{% img /uploads/images/ml01/mv.png 800 'model and variance' %}

Q: WHY?
A: 因為越簡單的 model 受到不同 data 的影響較小。

## Model vs Bias
> 以 model 複雜度來說，越簡單 bias 越大。

$E[f^*] = \bar{f}$

若 bias 大，表示 $\bar{f}$ 與 $\hat{f}$ 有距離。
若 bias 小，表示 $\bar{f}$ 與 $\hat{f}$ 是靠近的。

1. black curve: the true function $\hat{f}$
2. red curves: 5000 $f^*$
3. blue curve: average of 5000 $f^* = \bar{f}$

若為一次方的 model：
{% img /uploads/images/ml01/mb1.png 800 'model and bias' %}
$\bar{f}$ 與 $\hat{f}$ 較遠。

若為五次方的 model：
{% img /uploads/images/ml01/mb2.png 800 'model and bias' %}
$\bar{f}$ 與 $\hat{f}$ 是接近的。

